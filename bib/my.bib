@article{10.1145/3580784,
  author     = {Nakamura, Yugo and Nakaoka, Rei and Matsuda, Yuki and Yasumoto, Keiichi},
  title      = {Eat2pic: An Eating-Painting Interactive System to Nudge Users into Making Healthier Diet Choices},
  year       = {2023},
  issue_date = {March 2023},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {7},
  number     = {1},
  url        = {https://doi.org/10.1145/3580784},
  doi        = {10.1145/3580784},
  abstract   = {Given the complexity of human eating behaviors, developing interactions to change the way users eat or their choice of meals is challenging. In this study, we propose an interactive system called eat2pic designed to encourage healthy eating habits such as adopting a balanced diet and eating more slowly, by refraining the task of selecting meals into that of adding color to landscape pictures. The eat2pic system comprises a sensor-equipped chopstick (one of a pair) and two types of digital canvases. It provides fast feedback by recognizing a user's eating behavior in real time and displaying the result on a small canvas called "one-meal eat2pic." Moreover, it also provides slow feedback by displaying the number of colors of foods that the user consumed on a large canvas called "one-week eat2pic." The former was designed and implemented as a guide to help people eat more slowly, and the latter to encourage people to select more balanced menus. Through two user studies, we explored the experience of interaction with eat2pic, in which users' daily eating behavior was reflected in a series of "paintings," that is, images produced by the automated system. The experimental results suggest that eat2pic may provide an opportunity for reflection in meal selection and while eating, as well as assist users in becoming more aware of how they are eating and how balanced their daily meals are. We expect this system to inspire users' curiosity about different diets and ways of eating. This research also contributes to expanding the design space for products and services related to dietary support.},
  journal    = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  month      = {mar},
  articleno  = {24},
  numpages   = {23},
  keywords   = {Well-being, Human-food interaction, Aesthetic feedback, Digital nudge, Dietary monitoring, Behavior change}
}

@article{bitescan,
  title   = {ウェアラブル・デバイスによる咀嚼回数計測},
  author  = {山鹿 義郎 and 堀 一浩 and 上原 文子 and 谷村 基樹 and 齋藤 功 and 小野 高裕},
  journal = {日本顎口腔機能学会雑誌},
  volume  = {24},
  number  = {2},
  pages   = {116-117},
  year    = {2018},
  doi     = {10.7144/sgf.24.116}
}

@article{tonyobyo53112,
  title   = {糖尿病患者における食品の摂取順序による食後血糖上昇抑制効果},
  author  = {今井 佐恵子 and 松田 美久子 and 藤本 さおり and 宮谷 秀一 and 長谷川 剛二 and 福井 道明 and 森上 眞弓 and 小笹 寧子 and 梶山 静夫},
  journal = {糖尿病},
  volume  = {53},
  number  = {2},
  pages   = {112-115},
  year    = {2010},
  doi     = {10.11213/tonyobyo.53.112}
}

@article{10.2337/dc15-0429,
  author   = {Shukla, Alpana P. and Iliescu, Radu G. and Thomas, Catherine E. and Aronne, Louis J.},
  title    = {Food Order Has a Significant Impact on Postprandial Glucose and Insulin Levels},
  journal  = {Diabetes Care},
  volume   = {38},
  number   = {7},
  pages    = {e98-e99},
  year     = {2015},
  month    = {06},
  abstract = {Postprandial hyperglycemia is an important therapeutic target for optimizing glycemic control and for mitigating the proatherogenic vascular environment characteristic of type 2 diabetes. Existing evidence indicates that the quantity and type of carbohydrate consumed influence blood glucose levels and that the total amount of carbohydrate consumed is the primary predictor of glycemic response (1). Previous studies have shown that premeal ingestion of whey protein, as well as altering the macronutrient composition of a meal, reduces postmeal glucose levels (2–4). There are limited data, however, regarding the effect of food order on postprandial glycemia in patients with type 2 diabetes (5). In this pilot study, we sought to examine the effect of food order, using a typical Western meal, incorporating vegetables, protein, and carbohydrate, on postprandial glucose and insulin excursions in overweight/obese adults with type 2 diabetes.},
  issn     = {0149-5992},
  doi      = {10.2337/dc15-0429},
  url      = {https://doi.org/10.2337/dc15-0429},
  eprint   = {https://diabetesjournals.org/care/article-pdf/38/7/e98/623593/dc150429.pdf}
}

@article{yabe2019107450,
  title   = {Dietary instructions focusing on meal-sequence and nutritional balance for prediabetes subjects: An exploratory, cluster-randomized, prospective, open-label, clinical trial},
  journal = {Journal of Diabetes and its Complications},
  volume  = {33},
  number  = {12},
  pages   = {107450},
  year    = {2019},
  issn    = {1056-8727},
  doi     = {https://doi.org/10.1016/j.jdiacomp.2019.107450},
  url     = {https://www.sciencedirect.com/science/article/pii/S105687271930621X},
  author  = {Daisuke Yabe and Hitoshi Kuwata and Yuuka Fujiwara and Mayuka Sakaguchi and Shota Moyama and Noboru Makabe and Kenta Murotani and Hiroshi Asano and Sanae Ito and Hideyuki Mishima and Hideto Takase and Noriyasu Ota and Yusuke Seino and Yoshiyuki Hamamoto and Takeshi Kurose and Yutaka Seino}
}

@article{20249,
  title  = {Abstract 20249: Slow Down, You Eat Too Fast: Fast Eating Associate With Obesity and Future Prevalence of Metabolic Syndrome},
  author = {Takayuki Yamaji and Shinsuke Mikami and Hiroshi Kobatake and Koichi Tanaka Yukihito Higashi and Yasuki Kihara},
  year   = {2018},
  url    = {https://www.ahajournals.org/doi/abs/10.1161/circ.136.suppl_1.20249}
}

@misc{beyond_willpower,
  author       = {Catapult},
  title        = {Beyond Willpower: Diet Quality and Quantity Matter},
  howpublished = {"\url{https://www.hsph.harvard.edu/obesity-prevention-source/obesity-causes/diet-and-weight/}"，（2023年6月27日閲覧）}
}

@article{bitescan,
  title   = {ウェアラブル・デバイスによる咀嚼回数計測},
  author  = {山鹿 義郎 and 堀 一浩 and 上原 文子 and 谷村 基樹 and 齋藤 功 and 小野 高裕},
  journal = {日本顎口腔機能学会雑誌},
  volume  = {24},
  number  = {2},
  pages   = {116-117},
  year    = {2018},
  doi     = {10.7144/sgf.24.116}
}

@inproceedings{10.1145/3551626.3564964,
  author    = {Sato, Kenshiro and Yamakata, Yoko and Amano, Sosuke and Aizawa, Kiyoharu},
  title     = {Wearable Camera Based Food Logging System},
  year      = {2022},
  isbn      = {9781450394789},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3551626.3564964},
  doi       = {10.1145/3551626.3564964},
  abstract  = {Recently, meal management apps have allowed people to record food items and calories from photos automatically. These technologies include extracting food regions from photos of served meals, identifying the name of the food in each region, and calculating nutritional data. However, what you eat is not the only indicator that should be kept in the food record. How fast you eat and the order in which you eat is also significant information for dietary management. Therefore, we aim to construct a system that automatically generates a meal log from first-person videos that users capture of their eating behavior with a wearable camera. To tackle the complex problems that the data this system assumes contains, we constructed an eating behavior record dataset: 9.9 hours of first-person video that assume the natural diets of a user. To investigate the feasibility of our proposed system, we evaluated whether the first step, the detection of the meal area in the video during the meal, could be achieved with sufficient accuracy using this dataset. Using the limited number of frames assumed to be annotated by the user as training data, 30 frames were annotated for user-specific model training and four frames for online adaptation, resulting in detection accuracy of 72\% for food regions. Our next goal is to create a multi-user dataset and service the application.},
  booktitle = {Proceedings of the 4th ACM International Conference on Multimedia in Asia},
  articleno = {33},
  numpages  = {5},
  keywords  = {first-person video, dietary record video dataset, eating behavior},
  location  = {Tokyo, Japan},
  series    = {MMAsia '22}
}

@article{10.1145/3063592,
  author     = {Pouladzadeh, Parisa and Shirmohammadi, Shervin},
  title      = {Mobile Multi-Food Recognition Using Deep Learning},
  year       = {2017},
  issue_date = {August 2017},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {13},
  number     = {3s},
  issn       = {1551-6857},
  url        = {https://doi.org/10.1145/3063592},
  doi        = {10.1145/3063592},
  abstract   = {In this article, we propose a mobile food recognition system that uses the picture of the food, taken by the user’s mobile device, to recognize multiple food items in the same meal, such as steak and potatoes on the same plate, to estimate the calorie and nutrition of the meal. To speed up and make the process more accurate, the user is asked to quickly identify the general area of the food by drawing a bounding circle on the food picture by touching the screen. The system then uses image processing and computational intelligence for food item recognition. The advantage of recognizing items, instead of the whole meal, is that the system can be trained with only single item food images. At the training stage, we first use region proposal algorithms to generate candidate regions and extract the convolutional neural network (CNN) features of all regions. Second, we perform region mining to select positive regions for each food category using maximum cover by our proposed submodular optimization method. At the testing stage, we first generate a set of candidate regions. For each region, a classification score is computed based on its extracted CNN features and predicted food names of the selected regions. Since fast response is one of the important parameters for the user who wants to eat the meal, certain heavy computational parts of the application are offloaded to the cloud. Hence, the processes of food recognition and calorie estimation are performed in cloud server. Our experiments, conducted with the FooDD dataset, show an average recall rate of 90.98\%, precision rate of 93.05\%, and accuracy of 94.11\% compared to 50.8\% to 88\% accuracy of other existing food recognition systems.},
  journal    = {ACM Trans. Multimedia Comput. Commun. Appl.},
  month      = {aug},
  articleno  = {36},
  numpages   = {21},
  keywords   = {cloud computing, Mobile food recognition, deep learning}
}

@article{1523669555317207552,
  author    = {森本 雅和},
  title     = {生活支援 画像認識システムのレジ応用 : パン画像認識レジBakeryScanの開発と展開},
  issn      = {09156755},
  publisher = {日本工業出版},
  year      = {2019},
  month     = {03},
  volume    = {30},
  number    = {3},
  pages     = {43-47},
  url       = {https://cir.nii.ac.jp/crid/1523669555317207552}
}

@article{app12126135,
  author         = {Hussain, Ghulam and Ali Saleh Al-rimy, Bander and Hussain, Saddam and Albarrak, Abdullah M. and Qasem, Sultan Noman and Ali, Zeeshan},
  title          = {Smart Piezoelectric-Based Wearable System for Calorie Intake Estimation Using Machine Learning},
  journal        = {Applied Sciences},
  volume         = {12},
  year           = {2022},
  number         = {12},
  article-number = {6135},
  url            = {https://www.mdpi.com/2076-3417/12/12/6135},
  issn           = {2076-3417},
  abstract       = {Eating an appropriate food volume, maintaining the required calorie count, and making good nutritional choices are key factors for reducing the risk of obesity, which has many consequences such as Osteoarthritis (OA) that affects the patient&rsquo;s knee. In this paper, we present a wearable sensor in the form of a necklace embedded with a piezoelectric sensor, that detects skin movement from the lower trachea while eating. In contrast to the previous state-of-the-art piezoelectric sensor-based system that used spectral features, our system fully exploits temporal amplitude-varying signals for optimal features, and thus classifies foods more accurately. Through evaluation of the frame length and the position of swallowing in the frame, we found the best performance was with a frame length of 30 samples (1.5 s), with swallowing located towards the end of the frame. This demonstrates that the chewing sequence carries important information for classification. Additionally, we present a new approach in which the weight of solid food can be estimated from the swallow count, and the calorie count of food can be calculated from their estimated weight. Our system based on a smartphone app helps users live healthily by providing them with real-time feedback about their ingested food types, volume, and calorie count.},
  doi            = {10.3390/app12126135}
}

@inproceedings{Dossou_2021_ICCV,
  author    = {Dossou, Bonaventure F. P. and Gbenou, Yeno K. S.},
  title     = {FSER: Deep Convolutional Neural Networks for Speech Emotion Recognition},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
  month     = {October},
  year      = {2021},
  pages     = {3533-3538}
}

@article{726791,
  author  = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal = {Proceedings of the IEEE},
  title   = {Gradient-based learning applied to document recognition},
  year    = {1998},
  volume  = {86},
  number  = {11},
  pages   = {2278-2324},
  doi     = {10.1109/5.726791}
}

@inproceedings{10.1145/3341162.3343822,
  author    = {Hossain, Tahera and Islam, Md Shafiqul and Ahad, Md Atiqur Rahman and Inoue, Sozo},
  title     = {Human Activity Recognition Using Earable Device},
  year      = {2019},
  isbn      = {9781450368698},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3341162.3343822},
  doi       = {10.1145/3341162.3343822},
  abstract  = {Wearable sensors are monumental for human activity recognition. Researchers are continuously inventing new technology to detect human activity properly. Earable opens up interesting possibilities of monitoring personal scale behavioral activities. In this paper, we explore earables device 'eSense' multisensory stereo device for personal scale behavior analysis. We propose an activity recognition framework by exploiting eSense based multi-sensory device. It has a microphone, 6-axis inertial measurement unit, and a dual-mode Bluetooth. We use eSense accelerometer sensor data for detecting head and mouth related behavioral activities. We develop a data collection framework from the eSense through our smartphone application via Bluetooth. Then from the collected data, a few statistical features are computed to classify six personal scale activities related to head and neck movement such as speaking, eating, headshaking and head nodding, as well as, stay and walk. We aggregate the time series data intzo different action labels that summarize the user activity over a time interval. After, we train the data to induce a predictive model for activity recognition. We explore both machine learning and deep learning approach for data classification. For classification, we use the Support Vector Machine, Random Forest, and K-Nearest Neighbor and Convolutional Neural Network and achieve satisfactory recognition accuracy. The findings provide promising prospect for eSense for personal scale activity recognition in healthcare monitoring service. Based on our study, this kid of work is done for the first time with satisfactory findings.},
  booktitle = {Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers},
  pages     = {81–84},
  numpages  = {4},
  keywords  = {eSense, earables, activity recognition, healthcare, wearable},
  location  = {London, United Kingdom},
  series    = {UbiComp/ISWC '19 Adjunct}
}

@article{10.1145/3478085,
  author     = {Verma, Dhruv and Bhalla, Sejal and Sahnan, Dhruv and Shukla, Jainendra and Parnami, Aman},
  title      = {ExpressEar: Sensing Fine-Grained Facial Expressions with Earables},
  year       = {2021},
  issue_date = {Sept 2021},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {5},
  number     = {3},
  url        = {https://doi.org/10.1145/3478085},
  doi        = {10.1145/3478085},
  abstract   = {Continuous and unobtrusive monitoring of facial expressions holds tremendous potential to enable compelling applications in a multitude of domains ranging from healthcare and education to interactive systems. Traditional, vision-based facial expression recognition (FER) methods, however, are vulnerable to external factors like occlusion and lighting, while also raising privacy concerns coupled with the impractical requirement of positioning the camera in front of the user at all times. To bridge this gap, we propose ExpressEar, a novel FER system that repurposes commercial earables augmented with inertial sensors to capture fine-grained facial muscle movements. Following the Facial Action Coding System (FACS), which encodes every possible expression in terms of constituent facial movements called Action Units (AUs), ExpressEar identifies facial expressions at the atomic level. We conducted a user study (N=12) to evaluate the performance of our approach and found that ExpressEar can detect and distinguish between 32 Facial AUs (including 2 variants of asymmetric AUs), with an average accuracy of 89.9\% for any given user. We further quantify the performance across different mobile scenarios in presence of additional face-related activities. Our results demonstrate ExpressEar's applicability in the real world and open up research opportunities to advance its practical adoption.},
  journal    = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  month      = {sep},
  articleno  = {129},
  numpages   = {28},
  keywords   = {facial expressions, IMU sensing, FACS, earable computing}
}

@inproceedings{10.1145/3491102.3517698,
  author    = {Wang, Yuntao and Ding, Jiexin and Chatterjee, Ishan and Salemi Parizi, Farshid and Zhuang, Yuzhou and Yan, Yukang and Patel, Shwetak and Shi, Yuanchun},
  title     = {FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones},
  year      = {2022},
  isbn      = {9781450391573},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3491102.3517698},
  doi       = {10.1145/3491102.3517698},
  abstract  = {Face orientation can often indicate users’ intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user’s earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user’s face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri’s performance. The results show that the system can determine whether the user orients to the device at a 93.5\% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track user’s head orientation with a median absolute error of 10.9 mm in the distance, 3.7° in yaw, and 5.8° in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interactions.},
  booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  articleno = {290},
  numpages  = {12},
  keywords  = {head pose estimation., Acoustic ranging, earphone, head orientation},
  location  = {New Orleans, LA, USA},
  series    = {CHI '22}
}

@inproceedings{10.1145/3313831.3376836,
  author    = {Xu, Xuhai and Shi, Haitian and Yi, Xin and Liu, WenJia and Yan, Yukang and Shi, Yuanchun and Mariakakis, Alex and Mankoff, Jennifer and Dey, Anind K.},
  title     = {EarBuddy: Enabling On-Face Interaction via Wireless Earbuds},
  year      = {2020},
  isbn      = {9781450367080},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3313831.3376836},
  doi       = {10.1145/3313831.3376836},
  abstract  = {Past research regarding on-body interaction typically requires custom sensors, limiting their scalability and generalizability. We propose EarBuddy, a real-time system that leverages the microphone in commercial wireless earbuds to detect tapping and sliding gestures near the face and ears. We develop a design space to generate 27 valid gestures and conducted a user study (N=16) to select the eight gestures that were optimal for both human preference and microphone detectability. We collected a dataset on those eight gestures (N=20) and trained deep learning models for gesture detection and classification. Our optimized classifier achieved an accuracy of 95.3\%. Finally, we conducted a user study (N=12) to evaluate EarBuddy's usability. Our results show that EarBuddy can facilitate novel interaction and that users feel very positively about the system. EarBuddy provides a new eyes-free, socially acceptable input method that is compatible with commercial wireless earbuds and has the potential for scalability and generalizability},
  booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages     = {1–14},
  numpages  = {14},
  keywords  = {face and ear interaction, gesture recognition, wireless earbuds},
  location  = {Honolulu, HI, USA},
  series    = {CHI '20}
}

@inproceedings{10.1145/3460120.3485340,
  author    = {Wang, Zi and Ren, Yili and Chen, Yingying and Yang, Jie},
  title     = {Earable Authentication via Acoustic Toothprint},
  year      = {2021},
  isbn      = {9781450384544},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3460120.3485340},
  doi       = {10.1145/3460120.3485340},
  abstract  = {Earables (ear wearable) are rapidly emerging as a new platform to enable a variety of personal applications. The traditional authentication methods thus become less applicable and inconvenient for earables due to their limited input interface. Earables, however, often feature rich around the head sensing capability that can be leveraged to capture new types of biometrics. In this work, we propose ToothSonic that leverages the toothprint-induced sonic effect produced by a user performing teeth gestures for user authentication. In particular, we design several representative teeth gestures that can produce effective sonic waves carrying the information of the toothprint. To reliably capture the acoustic toothprint, it leverages the occlusion effect of the ear canal and the inward-facing microphone of the earables. It then extracts multi-level acoustic features to represent the intrinsic acoustic toothprint for authentication. The key advantages of ToothSonic are that it is suitable for earables and is resistant to various spoofing attacks as the acoustic toothprint is captured via the private teeth-ear channel of the user that is unknown to others. Our preliminary studies with 20 participants show that ToothSonic achieves 97\% accuracy with only three teeth gestures.},
  booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {2390–2392},
  numpages  = {3},
  keywords  = {toothprint, user authentication, biometrics, earable},
  location  = {Virtual Event, Republic of Korea},
  series    = {CCS '21}
}
